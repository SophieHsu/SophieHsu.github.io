<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Sophie (Ya-Chuan) Hsu</title>

    <meta name="author" content="Sophie (Ya-Chuan) Hsu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <!-- Dark mode -->
    <script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.5/lib/darkmode-js.min.js"></script>
    <script>
      var options = {
        autoMatchOsTheme: false // default: true
      }

      const darkmode = new Darkmode(options);
      darkmode.showWidget();
    </script>
    <table style="width:100%;max-width:880px;border:0px;border-spacing:0px;border-collapse:separate;margin:0 auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  <name>Sophie (Ya-Chuan) Hsu</name>
                </p>
                <p>I'm a 5th-year PhD candidate at USC, advised by Professor <a href="https://stefanosnikolaidis.net/">Stefanos Nikolaidis</a>. My research lies at the intersection of reinforcement learning (RL) and human-robot interaction (HRI). 
                </p>
                <p>
                  I am interested in enabling robots to effectively collaborate with humans by inferring human's intentions and reactions in uncertain and dynamic environments. 
                  At ICAROS lab, I study diverse collaboration behaviors, develop hierarchical POMDP frameworks for human-aware robotic planning under uncertainty, and infer human observation functions to improve robot decision-making in tasks. 
                  I have interned at Toyota Research Institute working 
                  with <a href="https://people.csail.mit.edu/rosman/">Guy Rosman</a>, <a href="https://scholar.google.com/citations?user=Pnbjx1AAAAAJ&hl=en">Jonathan DeCastro</a> and <a href="https://www.andrew-silva.com/">Andrew Silva</a> 
                  on designing an
                  <!-- human-reaction-time-aware  -->
                  assistive driving 
                  <!-- notification  -->
                  system.
                   <!-- using RL and LLMs. -->
                </p>

                <p>
                  Prior to joining USC, I completed my M.S. in Computer Science and Engineering at Texas A&M University, advised by Professor <a href="https://cse-robotics.engr.tamu.edu/dshell/">Dylan A. Shell</a>. where I worked on pedestrian-aware autonomous vehicle planning. I also work with Professor <a href="https://scholar.google.com/citations?user=PWmUzp0AAAAJ&hl=en">Swaminathan Gopalswam</a> and Professor <a href="https://scholar.google.com/citations?user=vnsFRJUAAAAJ&hl=en">Srikanth Saripall</a> at the Texas A&M Transportation Institute, formalizing human-machine communication protocols and deploying a behavior planner using ROS on a Ford Lincoln MKZ platform.
                </p>

                <p>Feel free to say hi: yachuanh at usc dot edu</p>

                <p style="text-align:center">
                  <!-- <a href="mailto:yachuan815@gmail.com">Email</a> &nbsp;/&nbsp; -->
                  <a href="data/Resume_Sophie-Hsu.pdf">CV</a> &nbsp;/&nbsp;
                  <!-- <a href="data/SophieHsu-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?user=40WQ9NwAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/SophieHsu13">Twitter</a> 
                  <!-- <a href="https://github.com/sophiehsu/">Github</a> -->
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/sophiehsu.jpeg" class="hoverZoomLink">
              </td>

            </tr>
          </tbody></table>
          
    </td></tr></tbody></table>
    
    <table style="width:100%;max-width:880px;border:0px;border-spacing:0px;border-collapse:separate;margin:0 auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Research</h2>
              <!-- <p>
                I'm interested in computer vision, deep learning, generative AI, and image processing. Most of my research is about inferring the physical world (shape, motion, color, light, etc) from images, usually with radiance fields. Some papers are <span class="highlight">highlighted</papertitle>.
              </p> -->
            </td>
          </tr>
        </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr onmouseout="cat3d_stop()" onmouseover="cat3d_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <!-- <div class="two" id='cat3d_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/overcooked.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div> -->
                  <img src='images/fov_2d.gif' width="160">
                  <img src='images/fov_baseline.gif' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="data/papers/2025_ICRA.pdf">
                <strong>Integrating Field of View in Human-Aware Collaborative Planning</strong>
                </a>
                <br>
                <strong><u>Ya-Chuan Hsu</u></strong>,
                Michael Defranco, 
                Rutvik Patel, 
                <a class="black_link" href="https://stefanosnikolaidis.net">Stefanos Nikolaidis</a>

                <br>
                <em>ICRA</em>, 2025 <!-- &nbsp <font color="red"><strong>(Oral Presentation)</strong></font> -->
                <br>
                <a href="data/papers/2025_ICRA.pdf">Paper</a> |
                <a href="https://github.com/SophieHsu/view-aware-hrc">Code</a>
                <p></p>
                <p>
                  This paper integrates human field-of-view (FOV) limitations into robot planning by adapting to the evolving subtask intent of humans based on their limited perception. To manage the resulting computational complexity, we propose a hierarchical online planner. In a steakhouse domain study, our FOV-aware planner reduced human interruptions and redundant actions. We further demonstrate our planner in a virtual reality kitchen environment.
                </p>
              </td>
            </tr>

            <tr onmouseout="cat3d_stop()" onmouseover="cat3d_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <!-- <div class="two" id='cat3d_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/overcooked.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div> -->
                  <img src='images/surrogate_model.png' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2304.13787">
                <papertitle>Surrogate Assisted Generation of Human-Robot Interaction Scenarios</papertitle>
                </a>
                <br>
                <a class="black_link" href="">Varun Bhatt</a>, 
                <a class="black_link" href="">Heramb Nemlekar</a>, 
                <a class="black_link" href="https://scholar.google.com/citations?user=RqSvzikAAAAJ&hl=en">Matthew C. Fontaine</a>,
                <a class="black_link" href="https://btjanaka.net/">Bryon Tjanaka</a>,
                <a class="black_link" href="">Hejia Zhang</a>, 
                <strong><u>Ya-Chuan Hsu</u></strong>, 
                <a class="black_link" href="https://stefanosnikolaidis.net">Stefanos Nikolaidis</a>

                <br>
                <em>CoRL</em>, 2023 <!-- &nbsp <font color="red"><strong>(Oral Presentation)</strong></font> -->
                <br>
                <a href="https://arxiv.org/pdf/2304.13787">arXiv</a>
                <p></p>
                <p>
                  We propose using surrogate models to efficiently generate diverse and reproducible failure scenarios in human-robot interaction tasks, reducing the computational cost of traditional simulation-based methods.
                </p>
              </td>
            </tr>

            <tr onmouseout="cat3d_stop()" onmouseover="cat3d_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <!-- <div class="two" id='cat3d_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/overcooked.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div> -->
                  <img src='images/divser_furniture.png' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2206.10608">
                <papertitle>Generating diverse indoor furniture arrangements</papertitle>
                </a>
                <br>
                <strong><u>Ya-Chuan Hsu</u></strong>, 
                <a class="black_link" href="https://scholar.google.com/citations?user=RqSvzikAAAAJ&hl=en">Matthew C. Fontaine</a>,
                <a class="black_link" href="">Sam Earle</a>, 
                <a class="black_link" href="">Maria Edwards</a>,
                <a class="black_link" href="">Julian Togelius</a>,
                <a class="black_link" href="https://stefanosnikolaidis.net">Stefanos Nikolaidis</a>

                <br>
                <em>SIGGRAPH Poster</em>, 2022 <!-- &nbsp <font color="red"><strong>(Oral Presentation)</strong></font> -->
                <br>
                <a href="https://arxiv.org/pdf/2206.10608">arXiv</a>
                <p></p>
                <p>
                  We propose a method using GANs and a quality diversity algorithm to generate realistic and diverse indoor furniture arrangements, varying in attributes like price and number of pieces.
                </p>
              </td>
            </tr>

            <tr onmouseout="cat3d_stop()" onmouseover="cat3d_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <!-- <div class="two" id='cat3d_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/overcooked.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div> -->
                  <img src='images/overcooked.png' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://overcooked-lsi.github.io/">
                <papertitle>On the Importance of Environments in Human-Robot Coordination</papertitle>
                </a>
                <br>
                <a class="black_link" href="https://scholar.google.com/citations?user=RqSvzikAAAAJ&hl=en">Matthew C. Fontaine</a>*,
                <strong><u>Ya-Chuan Hsu</u></strong>*, 
                <a class="black_link" href="https://yulunzhang.net/">Yulun Zhang</a>*, 
                <a class="black_link" href="https://btjanaka.net/">Bryon Tjanaka</a>,
                <a class="black_link" href="https://stefanosnikolaidis.net">Stefanos Nikolaidis</a>

                <br>
                <em>RSS</em>, 2021 <!-- &nbsp <font color="red"><strong>(Oral Presentation)</strong></font> -->
                <br>
                <a href="https://overcooked-lsi.github.io/">Project Page</a>
                /
                <a href="https://arxiv.org/abs/2106.10853">arXiv</a>
                <p></p>
                <p>
                  Research on human-robot collaboration often focuses on robot policies for fluent teamwork, overlooking the impact of the environment on coordination. We propose a framework for procedurally generating environments that are stylistically human-like, solvable by human-robot teams, and diverse in coordination behaviors.
                </p>
              </td>
            </tr>

            <tr onmouseout="cat3d_stop()" onmouseover="cat3d_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <!-- <div class="two" id='cat3d_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/c1.gif" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div> -->
                  <img src='images/c1.gif' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://cse-robotics.engr.tamu.edu/dshell/papers/iros2020pedestrian.pdf">
                <papertitle>A POMDP Treatment of Vehicle-Pedestrian Interaction: Implicit Coordination via Uncertainty-Aware Planning</papertitle>
                </a>
                <br>
                <strong><u>Ya-Chuan Hsu</u></strong>, 
                <a class="black_link" href="https://engineering.tamu.edu/mechanical/profiles/gopalswamy-s.html">Swaminathan Gopalswamy</a>, 
                <a class="black_link" href="https://engineering.tamu.edu/mechanical/profiles/saripalli.html">Srikanth Saripalli</a>,
                <a class="black_link" href="https://cse-robotics.engr.tamu.edu/dshell/">Dylan A Shell</a>

                <br>
                <em>IROS</em>, 2020 <!-- &nbsp <font color="red"><strong>(Oral Presentation)</strong></font> -->
                <br>
                <a href="https://cse-robotics.engr.tamu.edu/dshell/papers/iros2020pedestrian.pdf">Paper</a>
                <p></p>
                <p>
                  This paper tackles the challenge of resolving ambiguous traffic situations where autonomous vehicles cannot directly communicate intent. It proposes a model using a partially observable Markov decision process (POMDP) to produce changes in speed to express intent. The approach is validated in a simulated vehicle-pedestrian crossing and tested in real-world trials with a self-driving car, demonstrating safe and efficient navigation.
                </p>
              </td>
            </tr>

            <tr onmouseout="cat3d_stop()" onmouseover="cat3d_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <!-- <div class="two" id='cat3d_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/overcooked.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div> -->
                  <img src='images/vtc_image.png' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://cse-robotics.engr.tamu.edu/dshell/papers/vtc2018mdp.pdf">
                <papertitle>An MDP model of vehicle-pedestrian interaction at an unsignalized intersection</papertitle>
                </a>
                <br>
                <strong><u>Ya-Chuan Hsu</u></strong>, 
                <a class="black_link" href="https://engineering.tamu.edu/mechanical/profiles/gopalswamy-s.html">Swaminathan Gopalswamy</a>, 
                <a class="black_link" href="https://engineering.tamu.edu/mechanical/profiles/saripalli.html">Srikanth Saripalli</a>,
                <a class="black_link" href="https://cse-robotics.engr.tamu.edu/dshell/">Dylan A Shell</a>

                <br>
                <em>VTC-Fall</em>, 2018 <!-- &nbsp <font color="red"><strong>(Oral Presentation)</strong></font> -->
                <br>
                <a href="https://cse-robotics.engr.tamu.edu/dshell/papers/vtc2018mdp.pdf">Paper</a>
                <p></p>
                <p>
                  This paper is a preliminary study on communication between pedestrians and autonomous vehicles at unsignalized intersections. We propose a decision-theoretic model, using an MDP framework inspired by psychological studies, to represent pedestrian-vehicle interactions.
                </p>
              </td>
            </tr>
          </tbody></table>

          <!-- Service -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Service</h2>
                <li>Serving as a PhD mentor</strong> for the Women in Engineering (WiE) at USC.</li>
                <li>Serving/Served as a reviewer</strong> for ICRA 2025, HRI 2025, THRI 2024, THRI 2023, HRI 2022 (LBR), THRI 2021, HRI 2021, RSS 2021</li>
            </td>
          </tr>
          </tbody></table>
          
          <!-- Teaching -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Teaching</h2>
                <li> CSCI 545: Introduction to Robotics (Master's Level) &nbsp;&nbsp;-&nbsp;&nbsp; Fall 2021, 2023, 2024 </li> 
                <li> CSCI 641/699: Computational Human-Robot interaction (PhD Level) &nbsp;&nbsp;-&nbsp;&nbsp; Spring 2023, 2024 </li>
                <li> CSCI 170: Discrete Methods in Computer Science (Undergrad Level) &nbsp;&nbsp;-&nbsp;&nbsp; Spring 2021 </li>  
            </td>
          </tr>
          </tbody></table>

          <!-- <hr>
          <div style="margin:auto">
            <center>
              Inspired by <a href="https://jonbarron.info/">this</strong></a> and <a
                href="https://www.jessezhang.net/">this</strong></a>.
            </center>
          </div> -->
 
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <hr>
                <p style="text-align:center;font-size:small;">
                  Inspired by <a href="https://jonbarron.info/">this</strong></a> and <a
                href="https://www.jessezhang.net/">this</strong></a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
